#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on 2015-07-06 15:26:32
# Project: com21cn_v5

from pyspider.libs.base_handler import *
import re
import json
import urllib
from pyspider.image_server import *
import logging
logger = logging.getLogger("image_server")

class Handler(BaseHandler):
    crawl_config = {
        "headers":{
            "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.132 Safari/537.36",
        },
        "force_update":True,
    }

    @every(minutes=3 * 60)
    def on_start(self):
        self.crawl('http://auto.21cn.com/autonews/', callback=self.index_page)
        self.crawl('http://auto.21cn.com/autonews/list2.shtml', callback=self.index_page)
        self.crawl('http://auto.21cn.com/yongche/', callback=self.index_page)
        self.crawl('http://auto.21cn.com/yongche/list2.shtml', callback=self.index_page)
        self.crawl('http://auto.21cn.com/wenhua/', callback=self.index_page)
        self.crawl('http://auto.21cn.com/wenhua/list2.shtml', callback=self.index_page)
        self.crawl('http://auto.21cn.com/wenhua/shenghuo/', callback=self.index_page)
        self.crawl('http://auto.21cn.com/wenhua/shenghuo/list2.shtml', callback=self.index_page)
        self.crawl('http://auto.21cn.com/gouche/', callback=self.index_page)
        self.crawl('http://auto.21cn.com/gouche/list2.shtml', callback=self.index_page)
        self.crawl('http://auto.21cn.com/zixun/', callback=self.index_page)
        self.crawl('http://auto.21cn.com/zixun/list2.shtml', callback=self.index_page)
        self.crawl('http://auto.21cn.com/gouche/hangqing/', callback=self.index_page)
        self.crawl('http://auto.21cn.com/gouche/hangqing/list2.shtml', callback=self.index_page)
        self.crawl('http://auto.21cn.com/zixun/zhuanlan/', callback=self.index_page)
        self.crawl('http://auto.21cn.com/zixun/zhuanlan/list2.shtml', callback=self.index_page)
        
        
        
    @config(age=10 * 24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc('.clearfix > div > .bigPic > h3 > a[href^="http"]').items():
            self.crawl(each.attr.href, callback=self.detail_page)

    @config(priority=2)
    def detail_page(self, response):
        g = re.search(ur'作者：(.*)',response.doc('.infoAuthor').text())
        author = ""
        if g is not None:
            author = g.group(1)
        detail = {
            "url": response.url,
            "title": response.doc('.title').text(),
            "html": response.doc('#article_text').html(),
            "images": [x.attr("src") for x in response.doc("#article_text img").items()],
            "source": response.doc('td > a').text(),
            "date": response.doc('.pubTime').text(),
            "author": author,
        }
        next = response.doc('.page-ctrl a:last')
        if next.text() == u'下一页':
            self.crawl(next.attr.href, callback=self.next_page, save = detail)
        else:
            return self.extract_info(detail)
    
    @config(priority=1)
    def next_page(self, response):
        detail = response.save
        detail["html"] += response.doc('#article_text').html()
        detail["images"] += [x.attr("src") for x in response.doc("#article_text img").items()]
        next = response.doc('.page-ctrl a:last')
        if next.text() == u'下一页':
            self.crawl(next.attr.href, callback=self.next_page, save = detail)
        else:
           return self.extract_info(detail)
        
    def clean(self,h):
        r = re.compile(r'[\r\n\t]')
        h = r.sub('',h)
        r = re.compile(r'</?a.+?>')
        h = r.sub('',h)
        r = re.compile(r'<!--.*?-->')
        h = r.sub('',h)
        r = re.compile(r'<script>.*?</script>')
        h = r.sub('',h)
        r = re.compile(r'&#\d+;')
        h = r.sub('',h)
        r = re.compile(r'</?div.*?>')
        h = r.sub('',h)
        r = re.compile(r'</?personname.*?>')
        h = r.sub('',h)
        r = re.compile(r'</?b.*?>')
        h = r.sub('',h)
        r = re.compile(r'</?strong.*?>')
        h = r.sub('',h)
        r = re.compile(r'</?span.*?>')
        h = r.sub('',h)
        r = re.compile(r' (class|id|style|alt|data-link)=\".*?\"')
        h = r.sub('',h)
        r = re.compile(r'\s{2,}')
        h = r.sub('',h)
        r = re.compile(r'<p/>')
        h = r.sub('',h)
        r = re.compile(r'<p>\s*?</p>')
        h = r.sub('',h)
        r = re.compile(ur'\u3000')
        h = r.sub('',h)
        r = re.compile(ur'\u3000')
        h = r.sub('',h)
        r = re.compile(ur'\xa0')
        h = r.sub('',h)
        return h
    def extract_info(self, detail):
        ori_images = detail["images"]
        try:
            params = urllib.urlencode({"images":json.dumps(ori_images)})
            page = urllib.urlopen("%s?%s" % (image_server, params))
            rep = page.read()
            new_images = json.loads(rep)
            for i in range(0, len(ori_images)):
                detail["html"] = detail["html"].replace(ori_images[i], new_images[i])
        except Exception, e:
            rep = ori_images;
            logger.debug("image server exception:%s", e)
        detail["news"] = self.clean(detail["html"])
        detail["images"] = rep
        return detail
